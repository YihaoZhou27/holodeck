{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy as sp\n",
    "import h5py\n",
    "\n",
    "import holodeck as holo\n",
    "import holodeck.single_sources as ss\n",
    "from holodeck.constants import YR, MSOL\n",
    "from holodeck import utils, detstats, plot\n",
    "\n",
    "\n",
    "import hasasia.sensitivity as hsen\n",
    "import hasasia.sim as hsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in strains from hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sspath = '/Users/emigardiner/GWs/holodeck/output/2023-05-09-mbp-ss15_n100_r30_f100_d15_l5_p0/'\n",
    "hdfname = sspath+'ss_lib.hdf5'\n",
    "ssfile = h5py.File(hdfname, 'r')\n",
    "print(list(ssfile.keys()))\n",
    "fobs = ssfile['fobs'][:]\n",
    "dfobs = ssfile['dfobs'][:]\n",
    "hc_ss = ssfile['hc_ss'][...]\n",
    "hc_bg = ssfile['hc_bg'][...]\n",
    "ssfile.close()\n",
    "\n",
    "shape = hc_ss.shape\n",
    "nsamps, nfreqs, nreals, nlouds = shape[0], shape[1], shape[2], shape[3]\n",
    "print('N,F,R,L =',nsamps, nfreqs, nreals, nlouds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find frequency bin for 1/yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "fidx = (np.abs(fobs - 1/YR)).argmin()\n",
    "print('fobs[%d] = %f /yr' % (fidx, fobs[fidx]*YR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value to Match To\n",
    "The 12.5 yr paper quotes a media common process amplitude at f=1/yr of $A_\\mathrm{CP} \\cong 1.92\\times10^{-15}$ with fixed DE438 ephemeris, or $A_\\mathrm{CP} \\cong 1.53\\times10^{-15}$ with BAYESEPHEM corrections to DE438\n",
    "\n",
    "I know $h_c \\propto A_\\mathrm{yr^{-1}} (\\frac{f}{1 \\mathrm{yr^{-1}}})^{-2/3}$ but not sure about scale factor. If it's same as the amplitude we calculated earlier, $A = \\frac{10^{1/2}}{4} h_s = \\frac{10^{1/2}}{4} h_c * \\sqrt{\\Delta f_{obs} / f_{obs}} $ \n",
    "\n",
    "The current astro interp paper Eq. (17) uses turnover model eq.\n",
    "$$ h_c(f) = A_\\mathrm{yr} \\frac{(f\\cdot \\mathrm{yr})^{-2/3}}{(1+(f_b/f)^\\kappa)^{1/2}} $$\n",
    "\n",
    "15yr GWB paper uses strain amplitude $2.4 \\times 10^{-15}$\n",
    "\n",
    "So, I'll assume the 12.5 yr number is strain amplitude, and multiply that by $\\sqrt{\\frac{f}{df}}$ to get h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ref = 1.53*10**-15\n",
    "hc_ref = A_ref * np.sqrt(fobs[fidx]/dfobs[fidx])\n",
    "print(hc_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median strain at 1/yr amplitude, for each sample\n",
    "hc_1yr = np.sqrt(hc_bg[:,fidx,:]**2 + np.sum(hc_ss[:,fidx,:,:]**2, axis=-1)) # (N,R)\n",
    "hc_1yr = np.median(hc_1yr, axis=1) \n",
    "print(hc_1yr.shape)\n",
    "plt.plot(np.arange(nsamps), hc_1yr)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsort = np.argsort(np.abs(hc_1yr-hc_ref))\n",
    "\n",
    "plt.scatter(np.arange(nsamps), hc_1yr[nsort], s=3)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp_to_hc(amp_ref, fobs, dfobs):\n",
    "    hc = amp_ref*np.sqrt(fobs/dfobs)\n",
    "    return hc\n",
    "\n",
    "def rank_samples(hc_ss, hc_bg, fobs, dfobs, amp_ref):\n",
    "    \"\"\" Sort samples by those with f=1/yr char strains closest to some reference value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hc_ss : (N,F,R,L) NDarray\n",
    "        Characteristic strain of the loudest single sources.\n",
    "    hc_bg : (N,F,R) NDarray\n",
    "        Characteristic strain of the background.\n",
    "    fobs : (F,)\n",
    "        Observed GW frequency\n",
    "    dfobs : (F,)\n",
    "        Observed GW frequency bin widths.\n",
    "    amp_ref : scalar\n",
    "        Reference strain amplitude at f=1/yr\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nsort : (N,) 1Darray\n",
    "        Indices of the param space samples sorted by proximity to the reference 1yr amplitude.\n",
    "    \"\"\"\n",
    "    # find bin with 1/yr\n",
    "    fidx = (np.abs(fobs - 1/YR)).argmin()\n",
    "\n",
    "    # find reference (e.g. 12.5 yr) char strain\n",
    "    hc_ref = amp_to_hc(amp_ref, fobs[fidx], dfobs[fidx])\n",
    "\n",
    "    # select 1/yr median strains of samples\n",
    "    hc_1yr = np.sqrt(hc_bg[:,fidx,:]**2 + np.sum(hc_ss[:,fidx,:,:]**2, axis=-1)) # (N,R)\n",
    "    hc_1yr = np.median(hc_1yr, axis=1) \n",
    "\n",
    "    # sort by closest\n",
    "    nsort = np.argsort(np.abs(hc_1yr-hc_ref))\n",
    "\n",
    "    return nsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holo310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
